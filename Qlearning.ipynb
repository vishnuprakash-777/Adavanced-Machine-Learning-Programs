{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88oFhXk36Uei",
        "outputId": "7147831d-2f02-4382-94c1-b0a1e8385286"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100/1000 - Total reward: 0.0 - Epsilon: 0.6057704364907278\n",
            "Q-table snapshot:\n",
            "[[0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.1 0. ]\n",
            " [0.  0.  0.  0. ]]\n",
            "Episode 200/1000 - Total reward: 0.0 - Epsilon: 0.3669578217261671\n",
            "Q-table snapshot:\n",
            "[[0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.1 0. ]\n",
            " [0.  0.  0.  0. ]]\n",
            "Episode 300/1000 - Total reward: 0.0 - Epsilon: 0.22229219984074702\n",
            "Q-table snapshot:\n",
            "[[0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.1 0. ]\n",
            " [0.  0.  0.  0. ]]\n",
            "Episode 400/1000 - Total reward: 1.0 - Epsilon: 0.1346580429260134\n",
            "Q-table snapshot:\n",
            "[[4.16624354e-05 1.11640922e-01 8.98510995e-08 6.12862013e-03]\n",
            " [1.72441504e-06 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [9.11746245e-03 2.17254191e-01 0.00000000e+00 1.60066389e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [7.12983527e-04 0.00000000e+00 4.05640971e-01 0.00000000e+00]\n",
            " [4.53858892e-02 0.00000000e+00 6.29855332e-01 0.00000000e+00]\n",
            " [1.15596321e-01 8.11872353e-01 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.89019735e-01 9.57608842e-01 2.20678357e-02]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
            "Episode 500/1000 - Total reward: 0.0 - Epsilon: 0.0996820918179746\n",
            "Q-table snapshot:\n",
            "[[2.61006956e-01 9.38904116e-01 2.14110048e-02 9.77640781e-02]\n",
            " [2.07071999e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.70717089e-07 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.81140727e-01 9.55459911e-01 0.00000000e+00 2.07860545e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [2.22836815e-01 0.00000000e+00 9.68402727e-01 0.00000000e+00]\n",
            " [2.43098563e-01 0.00000000e+00 9.79580623e-01 0.00000000e+00]\n",
            " [3.35399999e-01 9.89894688e-01 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 2.69112264e-01 9.99989710e-01 2.03855300e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
            "Episode 600/1000 - Total reward: 1.0 - Epsilon: 0.0996820918179746\n",
            "Q-table snapshot:\n",
            "[[4.45297540e-01 9.50979521e-01 2.14110048e-02 3.87368592e-01]\n",
            " [2.07071999e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.70717089e-07 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [2.58103605e-01 9.60593523e-01 0.00000000e+00 4.06429929e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [2.96474221e-01 0.00000000e+00 9.70298440e-01 3.26686017e-01]\n",
            " [3.79422856e-01 0.00000000e+00 9.80099890e-01 0.00000000e+00]\n",
            " [4.56030301e-01 9.89999984e-01 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 4.64472836e-01 9.99999999e-01 4.70801546e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
            "Episode 700/1000 - Total reward: 0.0 - Epsilon: 0.0996820918179746\n",
            "Q-table snapshot:\n",
            "[[6.15934481e-01 9.50990047e-01 6.35637896e-02 4.42779748e-01]\n",
            " [3.46609452e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.70717089e-07 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [2.58103605e-01 9.60596009e-01 0.00000000e+00 4.59934704e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [4.76451225e-01 0.00000000e+00 9.70299000e-01 4.45303737e-01]\n",
            " [4.37540157e-01 0.00000000e+00 9.80100000e-01 0.00000000e+00]\n",
            " [5.95397116e-01 9.90000000e-01 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 6.06890697e-01 1.00000000e+00 4.70801546e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
            "Episode 800/1000 - Total reward: 1.0 - Epsilon: 0.0996820918179746\n",
            "Q-table snapshot:\n",
            "[[7.04157357e-01 9.50990050e-01 9.15217464e-02 5.77927556e-01]\n",
            " [4.06096522e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.70717089e-07 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [4.45875831e-01 9.60596010e-01 0.00000000e+00 5.08089249e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [5.68438734e-01 0.00000000e+00 9.70299000e-01 7.09121854e-01]\n",
            " [5.79288293e-01 0.00000000e+00 9.80100000e-01 0.00000000e+00]\n",
            " [6.66628474e-01 9.90000000e-01 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 6.45201628e-01 1.00000000e+00 6.79364356e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
            "Episode 900/1000 - Total reward: 1.0 - Epsilon: 0.0996820918179746\n",
            "Q-table snapshot:\n",
            "[[7.68471834e-01 9.50990050e-01 1.90511823e-01 6.47002549e-01]\n",
            " [5.51185485e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.70717089e-07 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [5.82761785e-01 9.60596010e-01 0.00000000e+00 6.85567157e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [6.42948617e-01 0.00000000e+00 9.70299000e-01 8.08169299e-01]\n",
            " [6.82622684e-01 5.68957851e-02 9.80100000e-01 0.00000000e+00]\n",
            " [7.24325874e-01 9.90000000e-01 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 4.63873410e-01 0.00000000e+00]\n",
            " [6.55490880e-02 6.45201628e-01 1.00000000e+00 7.09437920e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
            "Episode 1000/1000 - Total reward: 1.0 - Epsilon: 0.0996820918179746\n",
            "Q-table snapshot:\n",
            "[[8.27969394e-01 9.50990050e-01 1.90511823e-01 7.48273396e-01]\n",
            " [5.51185485e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.70717089e-07 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [6.52725155e-01 9.60596010e-01 0.00000000e+00 7.34190625e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [6.74713356e-01 0.00000000e+00 9.70299000e-01 8.08169299e-01]\n",
            " [7.57953456e-01 2.04182495e-01 9.80100000e-01 0.00000000e+00]\n",
            " [7.48923186e-01 9.90000000e-01 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 7.10394759e-01 0.00000000e+00]\n",
            " [1.99057238e-01 6.45201628e-01 1.00000000e+00 8.02518609e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
            "Average reward over 100 evaluation episodes: 1.0\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Initialize the FrozenLake environment\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
        "\n",
        "# Q-learning parameters\n",
        "alpha = 0.1  # Learning rate\n",
        "gamma = 0.99  # Discount factor\n",
        "epsilon = 1.0  # Exploration rate\n",
        "epsilon_min = 0.1  # Minimum exploration rate\n",
        "epsilon_decay = 0.995  # Decay rate for exploration probability\n",
        "\n",
        "# Initialize the Q-table\n",
        "q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "\n",
        "# Training parameters\n",
        "num_episodes = 1000\n",
        "max_steps_per_episode = 100\n",
        "\n",
        "# Q-learning algorithm\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    step = 0\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done and step < max_steps_per_episode:\n",
        "        # Exploration-exploitation tradeoff\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample()  # Explore\n",
        "        else:\n",
        "            action = np.argmax(q_table[state, :])  # Exploit\n",
        "\n",
        "        # Take the action and observe the outcome\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        # Update the Q-table\n",
        "        old_value = q_table[state, action]\n",
        "        next_max = np.max(q_table[next_state, :])\n",
        "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
        "        q_table[state, action] = new_value\n",
        "\n",
        "        state = next_state\n",
        "        step += 1\n",
        "        total_reward += reward\n",
        "\n",
        "    # Decay the exploration rate\n",
        "    if epsilon > epsilon_min:\n",
        "        epsilon *= epsilon_decay\n",
        "\n",
        "    if (episode + 1) % 100 == 0:\n",
        "        print(f'Episode {episode + 1}/{num_episodes} - Total reward: {total_reward} - Epsilon: {epsilon}')\n",
        "        print(f'Q-table snapshot:\\n{q_table}')\n",
        "\n",
        "# Evaluate the agent\n",
        "num_eval_episodes = 100\n",
        "total_rewards = 0\n",
        "\n",
        "for episode in range(num_eval_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    step = 0\n",
        "    episode_reward = 0\n",
        "\n",
        "    while not done and step < max_steps_per_episode:\n",
        "        action = np.argmax(q_table[state, :])  # Always exploit during evaluation\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        episode_reward += reward\n",
        "        state = next_state\n",
        "        step += 1\n",
        "\n",
        "    total_rewards += episode_reward\n",
        "\n",
        "average_reward = total_rewards / num_eval_episodes\n",
        "print(f'Average reward over {num_eval_episodes} evaluation episodes: {average_reward}')\n",
        "\n",
        "env.close()\n"
      ]
    }
  ]
}