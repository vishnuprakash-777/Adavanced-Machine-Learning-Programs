{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the grid size\n",
        "grid_size = 4\n",
        "\n",
        "# Define actions\n",
        "actions = ['up', 'down', 'left', 'right']\n",
        "\n",
        "# Define rewards\n",
        "rewards = np.zeros((grid_size, grid_size))\n",
        "rewards[3, 3] = 1  # Reward for reaching the goal state\n",
        "\n",
        "# Define transition probabilities\n",
        "transition_probs = {\n",
        "    'up': (lambda x, y: (max(x-1, 0), y)),\n",
        "    'down': (lambda x, y: (min(x+1, grid_size-1), y)),\n",
        "    'left': (lambda x, y: (x, max(y-1, 0))),\n",
        "    'right': (lambda x, y: (x, min(y+1, grid_size-1)))\n",
        "}\n",
        "\n",
        "def value_iteration(grid_size, rewards, transition_probs, actions, gamma=0.9, theta=1e-6):\n",
        "    value_table = np.zeros((grid_size, grid_size))\n",
        "    policy = np.zeros((grid_size, grid_size), dtype=int)\n",
        "\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for x in range(grid_size):\n",
        "            for y in range(grid_size):\n",
        "                if (x, y) == (3, 3):  # Skip the terminal state\n",
        "                    continue\n",
        "                v = value_table[x, y]\n",
        "                q_values = []\n",
        "                for i, action in enumerate(actions):\n",
        "                    (next_x, next_y) = transition_probs[action](x, y)\n",
        "                    q_value = rewards[x, y] + gamma * value_table[next_x, next_y]\n",
        "                    q_values.append(q_value)\n",
        "                value_table[x, y] = max(q_values)\n",
        "                policy[x, y] = np.argmax(q_values)\n",
        "                delta = max(delta, abs(v - value_table[x, y]))\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    return policy, value_table\n",
        "\n",
        "def print_policy(policy, actions):\n",
        "    policy_arrows = np.full(policy.shape, ' ')\n",
        "    for x in range(policy.shape[0]):\n",
        "        for y in range(policy.shape[1]):\n",
        "            if (x, y) == (3, 3):\n",
        "                policy_arrows[x, y] = 'G'  # Goal state\n",
        "            else:\n",
        "                policy_arrows[x, y] = actions[policy[x, y]][0].upper()\n",
        "    for row in policy_arrows:\n",
        "        print(' '.join(row))\n",
        "\n",
        "policy, value_table = value_iteration(grid_size, rewards, transition_probs, actions)\n",
        "print(\"Optimal Policy:\")\n",
        "print_policy(policy, actions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9prPNpRw0aas",
        "outputId": "4872107f-405d-4a78-c7cb-2584f26f406d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Policy:\n",
            "U U U U\n",
            "U U U U\n",
            "U U U U\n",
            "U U U G\n"
          ]
        }
      ]
    }
  ]
}